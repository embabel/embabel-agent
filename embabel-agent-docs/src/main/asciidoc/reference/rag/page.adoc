[[reference.rag]]
=== RAG (Retrieval-Augmented Generation)

Retrieval-Augmented Generation (RAG) is a technique that enhances LLM responses by retrieving relevant information from a knowledge base before generating answers.
This grounds LLM outputs in specific, verifiable sources rather than relying solely on training data.

For more background on RAG concepts, see:

- https://en.wikipedia.org/wiki/Retrieval-augmented_generation[Wikipedia: Retrieval-Augmented Generation]
- https://aws.amazon.com/what-is/retrieval-augmented-generation/[AWS: What is RAG?]

Embabel Agent provides RAG support through the `LlmReference` interface, which allows you to attach references (including RAG stores) to LLM calls.
The key classes are `ToolishRag` for exposing search operations as LLM tools, and `SearchOperations` for the underlying search functionality.

==== Agentic RAG Architecture

Unlike traditional RAG implementations that perform a single retrieval step, Embabel Agent's RAG is **entirely agentic and tool-based**.
The LLM has full control over the retrieval process:

- **Autonomous Search**: The LLM decides when to search, what queries to use, and how many results to retrieve
- **Iterative Refinement**: The LLM can perform multiple searches with different queries until it finds relevant information
- **Cross-Reference Discovery**: The LLM can follow references, expand chunks to see surrounding context, and zoom out to parent sections
- **HyDE Support**: The LLM can generate hypothetical documents (HyDE queries) to improve semantic search results

This agentic approach produces better results than single-shot RAG because the LLM can:

1. Start with a broad search and narrow down
2. Try different phrasings if initial queries return poor results
3. Expand promising results to get more context
4. Combine information from multiple chunks

==== Facade Pattern for Safe Tool Exposure

Embabel Agent uses a facade pattern to expose RAG capabilities safely and consistently across different store implementations.
The `ToolishRag` class acts as a facade that:

1. **Inspects Store Capabilities**: Examines which `SearchOperations` subinterfaces the store implements
2. **Exposes Appropriate Tools**: Only creates tool wrappers for supported operations
3. **Provides Consistent Interface**: All tools use the same parameter patterns regardless of underlying store

[source,kotlin]
----
override fun toolInstances(): List<Any> =
    buildList {
        if (searchOperations is VectorSearch) {
            add(VectorSearchTools(searchOperations))
        }
        if (searchOperations is TextSearch) {
            add(TextSearchTools(searchOperations))
        }
        if (searchOperations is ResultExpander) {
            add(ResultExpanderTools(searchOperations))
        }
        if (searchOperations is RegexSearchOperations) {
            add(RegexSearchTools(searchOperations))
        }
    }
----

This means:

- A Lucene store exposes vector search, text search, regex search, AND result expansion tools
- A Spring AI VectorStore adapter exposes only vector search tools
- A basic text-only store exposes only text search tools
- A directory-based text search exposes text search and regex search

The LLM sees only the tools that actually work with the configured store, preventing runtime errors from unsupported operations.

==== Getting Started

To use RAG in your Embabel Agent application, add the `rag-core` module and a store implementation to your `pom.xml`:

[source,xml]
----
<dependency>
    <groupId>com.embabel.agent</groupId>
    <artifactId>embabel-agent-rag-lucene</artifactId>
    <version>${embabel-agent.version}</version>
</dependency>

<dependency>
    <groupId>com.embabel.agent</groupId>
    <artifactId>embabel-agent-rag-tika</artifactId>
    <version>${embabel-agent.version}</version>
</dependency>
----

The `embabel-agent-rag-lucene` module provides Lucene-based vector and text search.
The `embabel-agent-rag-tika` module provides Apache Tika integration for parsing various document formats.

==== Our Model

Embabel Agent uses a hierarchical content model that goes beyond traditional flat chunk storage:

===== Content Elements

The `ContentElement` interface is the supertype for all content in the RAG system.
Key subtypes include:

- **`ContentRoot`** / **`NavigableDocument`**: The root of a document hierarchy, with a required URI and title
- **`Section`**: A hierarchical division of content with a title
- **`ContainerSection`**: A section containing other sections
- **`LeafSection`**: A section containing actual text content
- **`Chunk`**: Traditional RAG text chunks, created by splitting `LeafSection` content

===== Chunks

`Chunk` is the primary unit for vector search.
Each chunk:

- Contains a `text` field with the content
- Has a `parentId` linking to its source section
- Includes `metadata` with information about its origin (root document, container section, leaf section)
- Can compute its `pathFromRoot` through the document hierarchy

This hierarchical model enables advanced RAG capabilities like "zoom out" to parent sections or expansion to adjacent chunks.

==== SearchOperations

`SearchOperations` is the tag interface for search functionality.
Concrete implementations implement one or more subinterfaces based on their capabilities.
This design allows stores to implement only what's natural and efficient for them—a vector database need not pretend to support full-text search, and a text search engine need not fake vector similarity.

===== VectorSearch

Classic semantic vector search:

[source,kotlin]
----
interface VectorSearch : SearchOperations {
    fun <T : Retrievable> vectorSearch(
        request: TextSimilaritySearchRequest,
        clazz: Class<T>,
    ): List<SimilarityResult<T>>
}
----

===== TextSearch

Full-text search using Lucene query syntax:

[source,kotlin]
----
interface TextSearch : SearchOperations {
    fun <T : Retrievable> textSearch(
        request: TextSimilaritySearchRequest,
        clazz: Class<T>,
    ): List<SimilarityResult<T>>
}
----

Supported query syntax includes:

- `+term` - term must appear
- `-term` - term must not appear
- `"phrase"` - exact phrase match
- `term*` - prefix wildcard
- `term~` - fuzzy match

===== ResultExpander

Expand search results to surrounding context:

[source,kotlin]
----
interface ResultExpander : SearchOperations {
    fun expandResult(
        id: String,
        method: Method,
        elementsToAdd: Int,
    ): List<ContentElement>
}
----

Expansion methods:

- `SEQUENCE` - expand to previous and next chunks
- `ZOOM_OUT` - expand to enclosing section

===== RegexSearchOperations

Pattern-based search across content:

[source,kotlin]
----
interface RegexSearchOperations : SearchOperations {
    fun <T : Retrievable> regexSearch(
        regex: Regex,
        topK: Int,
        clazz: Class<T>,
    ): List<SimilarityResult<T>>
}
----

Useful for finding specific patterns like error codes, identifiers, or structured content that doesn't match well with semantic or keyword search.

===== CoreSearchOperations

A convenience interface combining the most common search capabilities:

[source,kotlin]
----
interface CoreSearchOperations : VectorSearch, TextSearch
----

Stores that support both vector and text search can implement this single interface for convenience.

==== ToolishRag

`ToolishRag` is an `LlmReference` that exposes `SearchOperations` as LLM tools.
This gives the LLM fine-grained control over RAG searches.

===== Configuration

Create a `ToolishRag` by wrapping your `SearchOperations`:

[source,java]
----
public ChatActions(SearchOperations searchOperations) {
    this.toolishRag = new ToolishRag(
            "sources",
            "Sources for answering user questions",
            searchOperations
    );
}
----

===== Using with LLM Calls

Attach `ToolishRag` to an LLM call using `.withReference()`:

[source,java]
----
@Action(canRerun = true, trigger = UserMessage.class)
void respond(Conversation conversation, ActionContext context) {
    var assistantMessage = context.ai()
            .withLlm(properties.chatLlm())
            .withReference(toolishRag)
            .withTemplate("ragbot")
            .respondWithSystemPrompt(conversation, Map.of(
                    "properties", properties
            ));
    context.sendMessage(conversation.addMessage(assistantMessage));
}
----

Based on the capabilities of the underlying `SearchOperations`, `ToolishRag` exposes:

- **VectorSearchTools**: `vectorSearch(query, topK, threshold)` - semantic similarity search
- **TextSearchTools**: `textSearch(query, topK, threshold)` - BM25 full-text search with Lucene syntax
- **RegexSearchTools**: `regexSearch(regex, topK)` - pattern-based search using regular expressions
- **ResultExpanderTools**: `broadenChunk(chunkId, chunksToAdd)` - expand to adjacent chunks, `zoomOut(id)` - expand to parent section

The LLM autonomously decides when to use these tools based on user queries.

===== ToolishRag lifecycle

It is safe to create a `ToolishRag` instance and reuse across many LLM calls.
However, instances are not expensive to create, so you can create a new instance per LLM call.
You might choose to do this if you provide a `ResultListener`
that will collect queries and results for logging or analysis: for example, to track which queries were most useful for answering user questions and the complexity in terms of number of searches performed.
This can be useful for implementing a learning feedback loop, for example to discern which queries performed badly, indicating that content such as documentation needs to be enhanced.

===== Metadata Filtering

In multi-tenant applications or scenarios where searches should be scoped to specific data subsets, `ToolishRag` supports **metadata filtering**.
Filters are applied transparently to all searches—the LLM does not see or control them, ensuring security and data isolation.

====== Motivation

Consider a document management system where:

- Each document belongs to an owner (user or organization)
- Some documents are shared reference data accessible to all users
- The LLM should only search documents the current user is authorized to access

Without filtering, you would need separate RAG stores per user or risk data leakage.
With metadata filtering, a single `ToolishRag` instance can be scoped per-request to the current user's data.

====== MetadataFilter API

The `MetadataFilter` sealed class hierarchy provides type-safe filter expressions:

[cols="1,2,2"]
|===
|Filter Type |Description |Example

|`Eq`
|Equals
|`MetadataFilter.eq("owner", "alice")`

|`Ne`
|Not equals
|`MetadataFilter.ne("status", "deleted")`

|`Gt`, `Gte`
|Greater than (or equal)
|`MetadataFilter.gte("score", 0.8)`

|`Lt`, `Lte`
|Less than (or equal)
|`MetadataFilter.lt("priority", 5)`

|`In`
|Value in list
|`MetadataFilter.in("category", "tech", "science")`

|`Nin`
|Value not in list
|`MetadataFilter.nin("status", "deleted", "archived")`

|`Contains`
|String contains substring
|`MetadataFilter.contains("tags", "important")`

|`And`
|Logical AND
|`MetadataFilter.and(filter1, filter2)`

|`Or`
|Logical OR
|`MetadataFilter.or(filter1, filter2)`

|`Not`
|Logical NOT
|`MetadataFilter.not(filter)`
|===

====== Kotlin Operator Syntax

Kotlin users can use operator and infix functions for a more natural DSL syntax:

[source,kotlin]
----
import com.embabel.agent.rag.filter.MetadataFilter.Companion.eq
import com.embabel.agent.rag.filter.MetadataFilter.Companion.gte
import com.embabel.agent.rag.filter.MetadataFilter.Companion.ne

// Simple filter with not operator
val notDeleted = !eq("status", "deleted")

// Combine with infix 'and' and 'or'
val userAccess = eq("owner", userId) and gte("confidenceScore", 0.7)

// Complex expressions with grouping
val accessFilter = (eq("owner", userId) and ne("status", "deleted")) or
    eq("role", "admin")
----

This is equivalent to the Java-style:

[source,java]
----
MetadataFilter accessFilter = MetadataFilter.or(
    MetadataFilter.and(
        MetadataFilter.eq("owner", userId),
        MetadataFilter.ne("status", "deleted")
    ),
    MetadataFilter.eq("role", "admin")
);
----

===== Neo4j Cypher Filtering

When using Neo4j via the Drivine module, filters are automatically converted to Cypher WHERE clauses using `CypherFilterConverter`:

[source,kotlin]
----
// The filter is converted to Cypher WHERE clause automatically
val filter = eq("owner", "alice") and gte("confidenceScore", 0.7)

// In DrivineNamedEntityDataRepository:
val results = repository.vectorSearch(request, filter)
// Generates: WHERE (e.owner = $_filter_0) AND (e.confidenceScore >= $_filter_1) AND ...
----

The converter produces parameterized queries for safety and handles all filter types including nested logical expressions.
Both `DrivineStore` (for chunks) and `DrivineNamedEntityDataRepository` (for named entities) support native Cypher filtering.

====== Basic Usage

Apply a filter to scope all searches to a specific owner:

[source,java]
----
// Create a filter for the current user
MetadataFilter ownerFilter = MetadataFilter.eq("ownerId", currentUserId);

// Apply to ToolishRag - all searches will be filtered
ToolishRag scopedRag = toolishRag.withFilter(ownerFilter);

// Use in LLM call - LLM cannot see or bypass the filter
context.ai()
    .withReference(scopedRag)
    .respondWithSystemPrompt(conversation, Map.of());
----

====== Complex Filters

Combine filters for more sophisticated access control:

[source,java]
----
// User can access their own documents OR documents in their departments
MetadataFilter accessFilter = MetadataFilter.or(
    MetadataFilter.eq("ownerId", currentUserId),
    MetadataFilter.in("departmentId", userDepartmentIds)
);

ToolishRag scopedRag = toolishRag.withFilter(accessFilter);
----

[source,java]
----
// Organization-scoped with status restriction
MetadataFilter orgFilter = MetadataFilter.and(
    MetadataFilter.eq("orgId", currentOrgId),
    MetadataFilter.ne("status", "deleted"),
    MetadataFilter.gte("confidenceScore", 0.7)
);

ToolishRag scopedRag = toolishRag.withFilter(orgFilter);
----

====== Per-Request Scoping Pattern

A common pattern is to create a scoped `ToolishRag` per request in a web application:

[source,java]
----
@Action(trigger = UserMessage.class)
void respond(Conversation conversation, ActionContext context) {
    // Get current user from security context
    String userId = SecurityContextHolder.getContext()
        .getAuthentication().getName();

    // Create user-scoped RAG for this request
    ToolishRag userScopedRag = toolishRag.withFilter(
        MetadataFilter.eq("ownerId", userId)
    );

    context.ai()
        .withReference(userScopedRag)
        .withTemplate("assistant")
        .respondWithSystemPrompt(conversation, Map.of());
}
----

====== Backend Implementation

Metadata filters are translated to native query syntax where possible:

- **Spring AI VectorStore**: Translated to `Filter.Expression` for native filtering at the vector store level
- **Lucene**: Applied as post-filter with inflated `topK` to compensate
- **Custom stores**: Can implement native translation or use `InMemoryMetadataFilter` for post-filtering

The `InMemoryMetadataFilter` utility class provides a fallback for any store implementation:

[source,java]
----
// In your SearchOperations implementation
List<SimilarityResult<T>> results = performSearch(request);
return InMemoryMetadataFilter.filterResults(results, filter);
----

This ensures filtering works across all backends, with native optimization where available.

==== Ingestion

===== Document Parsing with Tika

Embabel Agent uses Apache Tika for document parsing. `TikaHierarchicalContentReader` reads various formats (Markdown, HTML, PDF, Word, etc.) and extracts a hierarchical structure:

[source,java]
----
@ShellMethod("Ingest URL or file path")
String ingest(@ShellOption(defaultValue = "./data/document.md") String location) {
    var uri = location.startsWith("http://") || location.startsWith("https://")
            ? location
            : Path.of(location).toAbsolutePath().toUri().toString();
    var ingested = NeverRefreshExistingDocumentContentPolicy.INSTANCE
            .ingestUriIfNeeded(
                    luceneSearchOperations,
                    new TikaHierarchicalContentReader(),
                    uri
            );
    return ingested != null ?
            "Ingested document with ID: " + ingested :
            "Document already exists, no ingestion performed.";
}
----

===== Chunking Configuration

Content is split into chunks with configurable parameters:

[source,yaml]
----
ragbot:
  chunker-config:
    max-chunk-size: 800
    overlap-size: 100
----

Configuration options:

- `maxChunkSize` - Maximum characters per chunk (default: 1500)
- `overlapSize` - Character overlap between consecutive chunks (default: 200)
- `includeSectionTitleInChunk` - Include section title in chunk text (default: true)

===== Using Docling for Markdown Conversion

While we believe that you should write your Gen AI *applications* in Java or Kotlin, ingestion is more in the realm of data science, and Python is indisputably strong in this area.

For complex documents like PDFs, consider using https://github.com/DS4SD/docling[Docling] to convert to Markdown first:

[source,bash]
----
docling https://example.com/document.pdf --from pdf --to md --output ./data
----

Markdown is easier to parse hierarchically and produces better chunks than raw PDF extraction.

==== Supported Stores

Embabel Agent provides several RAG store implementations:

===== Lucene (embabel-agent-rag-lucene)

Full-featured store with vector search, text search, and result expansion.
Supports both in-memory and file-based persistence:

[source,java]
----
@Bean
LuceneSearchOperations luceneSearchOperations(
        ModelProvider modelProvider,
        RagbotProperties properties) {
    var embeddingService = modelProvider.getEmbeddingService(
            DefaultModelSelectionCriteria.INSTANCE);
    return LuceneSearchOperations
            .withName("docs")
            .withEmbeddingService(embeddingService)
            .withChunkerConfig(properties.chunkerConfig())
            .withIndexPath(Paths.get("./.lucene-index"))  // File persistence
            .buildAndLoadChunks();
}
----

Omit `.withIndexPath()` for in-memory only storage.

===== Neo4j

Graph database store for RAG (available in separate modules `embabel-agent-rag-neo-drivine` and `embabel-agent-rag-neo-ogm`).
Ideal when you need graph relationships between content elements.

===== Spring AI VectorStore (SpringVectorStoreVectorSearch)

Adapter that wraps any Spring AI `VectorStore`, enabling use of any vector database Spring AI supports:

[source,kotlin]
----
class SpringVectorStoreVectorSearch(
    private val vectorStore: VectorStore,
) : VectorSearch {
    override fun <T : Retrievable> vectorSearch(
        request: TextSimilaritySearchRequest,
        clazz: Class<T>,
    ): List<SimilarityResult<T>> {
        val searchRequest = SearchRequest
            .builder()
            .query(request.query)
            .similarityThreshold(request.similarityThreshold)
            .topK(request.topK)
            .build()
        val results = vectorStore.similaritySearch(searchRequest)
        // ... convert results
    }
}
----

This allows integration with Pinecone, Weaviate, Milvus, Chroma, and other stores via Spring AI.

==== Implementing Your Own RAG Store

To implement a custom RAG store, implement only the `SearchOperations` subinterfaces that are natural and efficient for your store.
This is a key design principle: **stores should only implement what they can do well**.

For example:

- A **vector database** like Pinecone might implement only `VectorSearch` since that's its strength
- A **full-text search engine** might implement `TextSearch` and `RegexSearchOperations`
- A **hierarchical document store** might add `ResultExpander` for context expansion
- A **full-featured store** like Lucene can implement all interfaces

The `ToolishRag` facade automatically exposes only the tools that your store supports.
This means you don't need to provide stub implementations or throw "not supported" exceptions—simply don't implement interfaces that don't fit your store's capabilities.

[source,kotlin]
----
// A store that only supports vector search
class MyVectorOnlyStore : VectorSearch {
    override fun <T : Retrievable> vectorSearch(
        request: TextSimilaritySearchRequest,
        clazz: Class<T>,
    ): List<SimilarityResult<T>> {
        // Implement vector similarity search
    }
}

// A store that supports both vector and text search
class MyFullTextStore : VectorSearch, TextSearch {
    override fun <T : Retrievable> vectorSearch(
        request: TextSimilaritySearchRequest,
        clazz: Class<T>,
    ): List<SimilarityResult<T>> {
        // Implement vector similarity search
    }

    override fun <T : Retrievable> textSearch(
        request: TextSimilaritySearchRequest,
        clazz: Class<T>,
    ): List<SimilarityResult<T>> {
        // Implement full-text search
    }

    override val luceneSyntaxNotes: String = "Full Lucene syntax supported"
}
----

For ingestion support, extend `ChunkingContentElementRepository` to handle document storage and chunking.

==== Complete Example

See the https://github.com/embabel/rag-demo[rag-demo] project for a complete working example including:

- Lucene-based RAG store configuration
- Document ingestion via Tika
- Chatbot with RAG-powered responses
- Jinja prompt templates for system prompts
- Spring Shell commands for interactive testing
