[[reference.llms]]
=== Working with LLMs

Embabel supports any LLM supported by Spring AI.
In practice, this is just about any LLM.

==== Choosing an LLM

Embabel encourages you to think about LLM choice for every LLM invocation.
The `PromptRunner` interface makes this easy.
Because Embabel enables you to break agentic flows up into multiple action steps, each step can use a smaller, focused prompt with fewer tools.
This means it may be able to use a smaller LLM.

Considerations:

- **Consider the complexity of the return type you expect** from the LLM.
This is typically a good proxy for determining required LLM quality.
A small LLM is likely to struggle with a deeply nested return structure.
- **Consider the nature of the task.** LLMs have different strengths; review any available documentation.
You don't necessarily need a huge, expensive model that is good at nearly everything, at the cost of your wallet and the environment.
- **Consider the sophistication of tool calling required**.
Simple tool calls are fine, but complex orchestration is another indicator you'll need a strong LLM.
(It may also be an indication that you should create a more sophisticated flow using Embabel GOAP.)
- **Consider trying a local LLM** running under Ollama or Docker.

TIP: Trial and error is your friend.
Embabel makes it easy to switch LLMs; try the cheapest thing that could work and switch if it doesn't.


==== Advanced: Custom LLM Integration

Embabel's tool loop is framework-agnostic, allowing you to integrate any LLM provider by implementing the `LlmMessageSender` interface.
This is useful when:

- You want to use an LLM provider not supported by Spring AI
- You need custom request/response handling
- You're integrating with a proprietary or internal LLM service

===== The LlmMessageSender Interface

The core abstraction is the `LlmMessageSender` functional interface:

[tabs]
====
Java::
+
[source,java]
----
@FunctionalInterface
public interface LlmMessageSender {
    LlmMessageResponse call(
        List<Message> messages,
        List<Tool> tools
    );
}
----

Kotlin::
+
[source,kotlin]
----
fun interface LlmMessageSender {
    fun call(
        messages: List<Message>,
        tools: List<Tool>,
    ): LlmMessageResponse
}
----
====

The implementation makes a single LLM inference call and returns the response.
Importantly, it does **not** execute tools--it only returns any tool call requests from the LLM.
Tool execution is handled by Embabel's `DefaultToolLoop`.

===== Response Types

The `LlmMessageResponse` contains:

- `message`: The LLM's response as an Embabel `Message`
- `textContent`: Text content from the response
- `usage`: Optional token usage information

For responses that include tool calls, return an `AssistantMessageWithToolCalls`:

[tabs]
====
Java::
+
[source,java]
----
public record ToolCall(
    String id,         // Unique identifier for the tool call
    String name,       // Name of the tool to invoke
    String arguments   // JSON arguments for the tool
) {}
----

Kotlin::
+
[source,kotlin]
----
data class ToolCall(
    val id: String,      // Unique identifier for the tool call
    val name: String,    // Name of the tool to invoke
    val arguments: String, // JSON arguments for the tool
)
----
====

===== Example: Custom LLM Provider

Here's an example of implementing `LlmMessageSender` for a hypothetical HTTP-based LLM API:

[tabs]
====
Java::
+
[source,java]
----
public class MyCustomLlmMessageSender implements LlmMessageSender {

    private final HttpClient httpClient;
    private final String apiKey;
    private final String model;

    public MyCustomLlmMessageSender(HttpClient httpClient, String apiKey, String model) {
        this.httpClient = httpClient;
        this.apiKey = apiKey;
        this.model = model;
    }

    @Override
    public LlmMessageResponse call(List<Message> messages, List<Tool> tools) {
        // Convert Embabel messages to your API's format
        List<Map<String, Object>> apiMessages = messages.stream()
            .map(message -> Map.<String, Object>of(
                "role", message.getRole().name().toLowerCase(),
                "content", message.getTextContent()
            ))
            .toList();

        // Convert tool definitions to your API's format
        List<Map<String, Object>> apiTools = tools.stream()
            .map(tool -> Map.<String, Object>of(
                "name", tool.getDefinition().getName(),
                "description", tool.getDefinition().getDescription(),
                "parameters", tool.getDefinition().getInputSchema().jsonSchema()
            ))
            .toList();

        // Make API request (using your preferred HTTP client)
        MyApiResponse responseBody = httpClient.post("https://api.my-llm.com/chat")
            .header("Authorization", "Bearer " + apiKey)
            .body(Map.of(
                "model", model,
                "messages", apiMessages,
                "tools", apiTools.isEmpty() ? null : apiTools
            ))
            .execute(MyApiResponse.class);

        // Check if LLM requested tool calls
        List<ToolCall> toolCalls = null;
        if (responseBody.getToolCalls() != null) {
            toolCalls = responseBody.getToolCalls().stream()
                .map(call -> new ToolCall(
                    call.getId(),
                    call.getFunction().getName(),
                    call.getFunction().getArguments()
                ))
                .toList();
        }

        Message embabelMessage;
        if (toolCalls == null || toolCalls.isEmpty()) {
            embabelMessage = new AssistantMessage(
                responseBody.getContent() != null ? responseBody.getContent() : ""
            );
        } else {
            embabelMessage = new AssistantMessageWithToolCalls(
                responseBody.getContent() != null ? responseBody.getContent() : "",
                toolCalls
            );
        }

        Usage usage = null;
        if (responseBody.getUsage() != null) {
            usage = new Usage(
                responseBody.getUsage().getPromptTokens(),
                responseBody.getUsage().getCompletionTokens()
            );
        }

        return new LlmMessageResponse(embabelMessage, responseBody.getContent(), usage);
    }
}
----

Kotlin::
+
[source,kotlin]
----
class MyCustomLlmMessageSender(
    private val httpClient: HttpClient,
    private val apiKey: String,
    private val model: String,
) : LlmMessageSender {

    override fun call(
        messages: List<Message>,
        tools: List<Tool>,
    ): LlmMessageResponse {
        // Convert Embabel messages to your API's format
        val apiMessages = messages.map { message ->
            mapOf(
                "role" to message.role.name.lowercase(),
                "content" to message.textContent
            )
        }

        // Convert tool definitions to your API's format
        val apiTools = tools.map { tool ->
            mapOf(
                "name" to tool.definition.name,
                "description" to tool.definition.description,
                "parameters" to tool.definition.inputSchema.jsonSchema()
            )
        }

        // Make API request
        val response = httpClient.post("https://api.my-llm.com/chat") {
            header("Authorization", "Bearer $apiKey")
            body = mapOf(
                "model" to model,
                "messages" to apiMessages,
                "tools" to apiTools.ifEmpty { null }
            )
        }

        // Parse response and convert to Embabel types
        val responseBody = response.body<MyApiResponse>()

        // Check if LLM requested tool calls
        val toolCalls = responseBody.toolCalls?.map { call ->
            ToolCall(
                id = call.id,
                name = call.function.name,
                arguments = call.function.arguments
            )
        }

        val embabelMessage = if (toolCalls.isNullOrEmpty()) {
            AssistantMessage(responseBody.content ?: "")
        } else {
            AssistantMessageWithToolCalls(
                content = responseBody.content ?: "",
                toolCalls = toolCalls
            )
        }

        return LlmMessageResponse(
            message = embabelMessage,
            textContent = responseBody.content ?: "",
            usage = responseBody.usage?.let { u ->
                Usage(
                    inputTokens = u.promptTokens,
                    outputTokens = u.completionTokens,
                )
            }
        )
    }
}
----
====

===== Creating an LlmService

To make your custom LLM available through Embabel's `ModelProvider`, implement the `LlmService` interface:

[tabs]
====
Java::
+
[source,java]
----
public class MyCustomLlmService implements LlmService<MyCustomLlmService> {

    private final String name;
    private final String provider;
    private final HttpClient httpClient;
    private final String apiKey;
    private final LocalDate knowledgeCutoffDate;
    private final List<PromptContributor> promptContributors;
    private final PricingModel pricingModel;

    public MyCustomLlmService(
            String name,
            String provider,
            HttpClient httpClient,
            String apiKey,
            LocalDate knowledgeCutoffDate,
            PricingModel pricingModel) {
        this.name = name;
        this.provider = provider;
        this.httpClient = httpClient;
        this.apiKey = apiKey;
        this.knowledgeCutoffDate = knowledgeCutoffDate;
        this.promptContributors = knowledgeCutoffDate != null
            ? List.of(new KnowledgeCutoffDate(knowledgeCutoffDate))
            : List.of();
        this.pricingModel = pricingModel;
    }

    @Override
    public String getName() { return name; }

    @Override
    public String getProvider() { return provider; }

    @Override
    public LocalDate getKnowledgeCutoffDate() { return knowledgeCutoffDate; }

    @Override
    public List<PromptContributor> getPromptContributors() { return promptContributors; }

    @Override
    public PricingModel getPricingModel() { return pricingModel; }

    @Override
    public LlmMessageSender createMessageSender(LlmOptions options) {
        return new MyCustomLlmMessageSender(
            httpClient,
            apiKey,
            options.getModel() != null ? options.getModel() : name
        );
    }

    @Override
    public MyCustomLlmService withKnowledgeCutoffDate(LocalDate date) {
        return new MyCustomLlmService(name, provider, httpClient, apiKey, date, pricingModel);
    }

    @Override
    public MyCustomLlmService withPromptContributor(PromptContributor promptContributor) {
        var newContributors = new ArrayList<>(promptContributors);
        newContributors.add(promptContributor);
        return new MyCustomLlmService(
            name, provider, httpClient, apiKey, knowledgeCutoffDate,
            newContributors, pricingModel
        );
    }
}
----

Kotlin::
+
[source,kotlin]
----
data class MyCustomLlmService(
    override val name: String,
    override val provider: String,
    private val httpClient: HttpClient,
    private val apiKey: String,
    override val knowledgeCutoffDate: LocalDate? = null,
    override val promptContributors: List<PromptContributor> =
        buildList { knowledgeCutoffDate?.let { add(KnowledgeCutoffDate(it)) } },
    override val pricingModel: PricingModel? = null,
) : LlmService<MyCustomLlmService> {

    override fun createMessageSender(options: LlmOptions): LlmMessageSender {
        return MyCustomLlmMessageSender(
            httpClient = httpClient,
            apiKey = apiKey,
            model = options.model ?: name,
        )
    }

    override fun withKnowledgeCutoffDate(date: LocalDate): MyCustomLlmService =
        copy(
            knowledgeCutoffDate = date,
            promptContributors = promptContributors + KnowledgeCutoffDate(date)
        )

    override fun withPromptContributor(promptContributor: PromptContributor): MyCustomLlmService =
        copy(promptContributors = promptContributors + promptContributor)
}
----
====

Then register it as a Spring bean:

[tabs]
====
Java::
+
[source,java]
----
@Configuration
public class MyLlmConfiguration {

    @Bean
    public LlmService<?> myCustomLlm(
            HttpClient httpClient,
            @Value("${my-llm.api-key}") String apiKey) {
        return new MyCustomLlmService(
            "my-custom-model",
            "MyProvider",
            httpClient,
            apiKey,
            LocalDate.of(2024, 12, 1),
            null
        );
    }
}
----

Kotlin::
+
[source,kotlin]
----
@Configuration
class MyLlmConfiguration {

    @Bean
    fun myCustomLlm(
        httpClient: HttpClient,
        @Value("\${my-llm.api-key}") apiKey: String,
    ): LlmService<*> = MyCustomLlmService(
        name = "my-custom-model",
        provider = "MyProvider",
        httpClient = httpClient,
        apiKey = apiKey,
        knowledgeCutoffDate = LocalDate.of(2024, 12, 1),
    )
}
----
====

The bean will be automatically discovered and made available through the `ModelProvider`.
You can then use it by name or role:

[tabs]
====
Java::
+
[source,java]
----
// By name
@LlmCall(llm = "my-custom-model")
String myAction();

// Or by role (if configured)
@LlmCall(llm = "#best")
String myAction();
----

Kotlin::
+
[source,kotlin]
----
// By name
@LlmCall(llm = "my-custom-model")
fun myAction(): String

// Or by role (if configured)
@LlmCall(llm = "#best")
fun myAction(): String
----
====

===== Using Your Custom Implementation (Alternative)

If you need more control over the LLM operations layer itself, you can extend `ToolLoopLlmOperations`:

[tabs]
====
Java::
+
[source,java]
----
public class MyCustomLlmOperations extends ToolLoopLlmOperations {

    private final HttpClient httpClient;
    private final String apiKey;

    public MyCustomLlmOperations(
            HttpClient httpClient,
            String apiKey,
            ModelProvider modelProvider,
            ToolDecorator toolDecorator,
            Validator validator) {
        super(modelProvider, toolDecorator, validator);
        this.httpClient = httpClient;
        this.apiKey = apiKey;
    }

    @Override
    protected LlmMessageSender createMessageSender(LlmService<?> llm, LlmOptions options) {
        return new MyCustomLlmMessageSender(
            httpClient,
            apiKey,
            options.getModel() != null ? options.getModel() : "default-model"
        );
    }
}
----

Kotlin::
+
[source,kotlin]
----
class MyCustomLlmOperations(
    private val httpClient: HttpClient,
    private val apiKey: String,
    modelProvider: ModelProvider,
    toolDecorator: ToolDecorator,
    validator: Validator,
) : ToolLoopLlmOperations(
    modelProvider = modelProvider,
    toolDecorator = toolDecorator,
    validator = validator,
) {
    override fun createMessageSender(
        llm: LlmService<*>,
        options: LlmOptions,
    ): LlmMessageSender {
        return MyCustomLlmMessageSender(
            httpClient = httpClient,
            apiKey = apiKey,
            model = options.model ?: "default-model",
        )
    }
}
----
====

The `ToolLoopLlmOperations` base class provides several extension points:

- `createMessageSender()`: Create the LLM communication layer
- `createOutputConverter()`: Parse LLM responses into typed objects
- `sanitizeStringOutput()`: Clean up raw text responses
- `emitCallEvent()`: Emit observability events

===== Key Implementation Notes

1. **Tool calls are not executed by your sender.** Just return the tool call requests--Embabel's tool loop handles execution and continuation.

2. **Handle both tool and non-tool responses.** Return `AssistantMessage` for plain text, `AssistantMessageWithToolCalls` when the LLM wants to invoke tools.

3. **Include usage information when available.** This enables cost tracking and observability.

4. **Message types matter.** The tool loop expects specific message types:
   - `UserMessage`: User input
   - `SystemMessage`: System prompts
   - `AssistantMessage`: LLM text response
   - `AssistantMessageWithToolCalls`: LLM response with tool requests
   - `ToolResultMessage`: Result returned to LLM after tool execution

