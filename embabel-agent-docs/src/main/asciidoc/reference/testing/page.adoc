[[reference.testing]]
=== Testing

Like Spring, Embabel facilitates testing of user applications.
The framework provides comprehensive testing support for both unit and integration testing scenarios.

IMPORTANT: Building Gen AI applications is no different from building other software.
Testing is critical to delivering quality software and must be considered from the outset.

==== Unit Testing

Unit testing in Embabel enables testing individual agent actions without involving real LLM calls.

Embabel's design means that agents are usually POJOs that can be instantiated with fake or mock objects.
Actions are methods that can be called directly with test fixtures.
In additional to your domain objects, you will pass a text fixture for the Embabel `OperationContext`, enabling you to intercept and verify LLM calls.

The framework provides `FakePromptRunner` and `FakeOperationContext` to mock LLM interactions while allowing you to verify prompts, hyperparameters, and business logic.
Alternatively you can use mock objects.
https://site.mockito.org/[Mockito] is the default choice for Java; https://mockk.io/[mockk] for Kotlin.

===== Java Example: Testing Prompts and Hyperparameters

Here's a unit test from the http://github.com/embabel/java-agent-template[Java Agent Template] repository, using Embabel fake objects:

[source,java]
----
class WriteAndReviewAgentTest {
    
    @Test
    void testWriteAndReviewAgent() {
        var context = FakeOperationContext.create();
        var promptRunner = (FakePromptRunner) context.promptRunner();
        context.expectResponse(new Story("One upon a time Sir Galahad . . "));

        var agent = new WriteAndReviewAgent(200, 400);
        agent.craftStory(new UserInput("Tell me a story about a brave knight", Instant.now()), context);

        String prompt = promptRunner.getLlmInvocations().getFirst().getPrompt();
        assertTrue(prompt.contains("knight"), "Expected prompt to contain 'knight'");

        var temp = promptRunner.getLlmInvocations().getFirst().getInteraction().getLlm().getTemperature();
        assertEquals(0.9, temp, 0.01,
                "Expected temperature to be 0.9: Higher for more creative output");
    }

    @Test
    void testReview() {
        var agent = new WriteAndReviewAgent(200, 400);
        var userInput = new UserInput("Tell me a story about a brave knight", Instant.now());
        var story = new Story("Once upon a time, Sir Galahad...");
        var context = FakeOperationContext.create();
        context.expectResponse("A thrilling tale of bravery and adventure!");
        agent.reviewStory(userInput, story, context);

        var promptRunner = (FakePromptRunner) context.promptRunner();
        String prompt = promptRunner.getLlmInvocations().getFirst().getPrompt();
        assertTrue(prompt.contains("knight"), "Expected review prompt to contain 'knight'");
        assertTrue(prompt.contains("review"), "Expected review prompt to contain 'review'");
    }
}
----

===== Kotlin Example: Testing Prompts and Hyperparameters

Here's the unit test from the http://github.com/embabel/kotlin-agent-template[Kotlin Agent Template] repository:

[source,kotlin]
----
/**
 * Unit tests for the WriteAndReviewAgent class.
 * Tests the agent's ability to craft and review stories based on user input.
 */
internal class WriteAndReviewAgentTest {

    /**
     * Tests the story crafting functionality of the WriteAndReviewAgent.
     * Verifies that the LLM call contains expected content and configuration.
     */
    @Test
    fun testCraftStory() {
        // Create agent with word limits: 200 min, 400 max
        val agent = WriteAndReviewAgent(200, 400)
        val context = FakeOperationContext.create()
        val promptRunner = context.promptRunner() as FakePromptRunner

        context.expectResponse(Story("One upon a time Sir Galahad . . "))

        agent.craftStory(
            UserInput("Tell me a story about a brave knight", Instant.now()),
            context
        )

        // Verify the prompt contains the expected keyword
        Assertions.assertTrue(
            promptRunner.llmInvocations.first().prompt.contains("knight"),
            "Expected prompt to contain 'knight'"
        )

        // Verify the temperature setting for creative output
        val actual = promptRunner.llmInvocations.first().interaction.llm.temperature
        Assertions.assertEquals(
            0.9, actual, 0.01,
            "Expected temperature to be 0.9: Higher for more creative output"
        )
    }

    @Test
    fun testReview() {
        val agent = WriteAndReviewAgent(200, 400)
        val userInput = UserInput("Tell me a story about a brave knight", Instant.now())
        val story = Story("Once upon a time, Sir Galahad...")
        val context = FakeOperationContext.create()
        
        context.expectResponse("A thrilling tale of bravery and adventure!")
        agent.reviewStory(userInput, story, context)

        val promptRunner = context.promptRunner() as FakePromptRunner
        val prompt = promptRunner.llmInvocations.first().prompt
        Assertions.assertTrue(prompt.contains("knight"), "Expected review prompt to contain 'knight'")
        Assertions.assertTrue(prompt.contains("review"), "Expected review prompt to contain 'review'")
        
        // Verify single LLM invocation during review
        Assertions.assertEquals(1, promptRunner.llmInvocations.size)
    }
}
----

===== Testing the Fluent API: withId() and creating()

The `FakePromptRunner` fully supports the fluent API patterns used in production code, enabling comprehensive unit testing of agents that use `withId()` for interaction tracing and `creating()` for structured object creation with examples.

**Testing withId() for Interaction Tracing:**

The `withId()` method sets an interaction ID for better log tracing. In tests, you can verify the interaction ID was correctly set:

[source,java]
----
@Test
void shouldSetInteractionIdCorrectly() {
    var context = FakeOperationContext.create();
    var expectedIntent = new UserIntent("command", "Change channel names");
    context.expectResponse(expectedIntent);

    var result = context.ai()
            .withId("classify-intent")  // Set interaction ID for tracing
            .creating(UserIntent.class)
            .fromPrompt("Classify the user's intent");

    assertEquals(expectedIntent, result);

    // Verify the interaction ID was set correctly
    var interaction = context.getLlmInvocations().getFirst().getInteraction();
    assertEquals("classify-intent", interaction.getId().getValue());
}
----

[source,kotlin]
----
@Test
fun `should set interaction ID correctly`() {
    val context = FakeOperationContext.create()
    val expectedIntent = UserIntent("command", "Change channel names")
    context.expectResponse(expectedIntent)

    val result = context.ai()
        .withId("classify-intent")  // Set interaction ID for tracing
        .creating(UserIntent::class.java)
        .fromPrompt("Classify the user's intent")

    assertEquals(expectedIntent, result)

    // Verify the interaction ID was set correctly
    val interaction = context.llmInvocations.first().interaction
    assertEquals(InteractionId("classify-intent"), interaction.id)
}
----

**Testing creating() with withExample():**

The `creating()` API allows you to provide strongly-typed examples to improve LLM output quality. In tests, you can verify examples were included:

[source,java]
----
@Test
void shouldIncludeExamplesInPrompt() {
    var context = FakeOperationContext.create();
    var expectedPlan = new ChannelEditPlan(1, "Lead Vox");
    context.expectResponse(expectedPlan);

    var result = context.ai()
            .withLlm(llmSelectionService.selectOptimalLlm())
            .withId("analyze-edit-request")
            .creating(ChannelEditPlan.class)
            .withExample("Rename channel 1", new ChannelEditPlan(1, "Bass"))
            .withExample("Rename channel 2", new ChannelEditPlan(2, "Drums"))
            .fromPrompt("Analyze the edit request");

    assertEquals(expectedPlan, result);

    // Verify examples were added as prompt contributors
    var promptContributors = context.getLlmInvocations().getFirst()
            .getInteraction().getPromptContributors();
    assertTrue(promptContributors.size() >= 2, "Examples should be added as prompt contributors");
}
----

[source,kotlin]
----
@Test
fun `should include examples in prompt`() {
    val context = FakeOperationContext.create()
    val expectedPlan = ChannelEditPlan(1, "Lead Vox")
    context.expectResponse(expectedPlan)

    val result = context.ai()
        .withLlm(llmSelectionService.selectOptimalLlm())
        .withId("analyze-edit-request")
        .creating(ChannelEditPlan::class.java)
        .withExample("Rename channel 1", ChannelEditPlan(1, "Bass"))
        .withExample("Rename channel 2", ChannelEditPlan(2, "Drums"))
        .fromPrompt("Analyze the edit request")

    assertEquals(expectedPlan, result)

    // Verify examples were added as prompt contributors
    val promptContributors = context.llmInvocations.first().interaction.promptContributors
    assertTrue(promptContributors.size >= 2, "Examples should be added as prompt contributors")
}
----

**Using ObjectCreationExample for Reusable Examples:**

For cleaner code and reusability, you can use the `ObjectCreationExample` data class to define examples that can be shared across tests or passed as collections:

[source,java]
----
@Test
void shouldUseObjectCreationExampleDataClass() {
    var context = FakeOperationContext.create();
    var expectedPlan = new ChannelEditPlan(1, "Lead Vox");
    context.expectResponse(expectedPlan);

    // Create a reusable example using ObjectCreationExample
    var example = new ObjectCreationExample<>(
        "Rename channel example",
        new ChannelEditPlan(2, "Rhythm")
    );

    var result = context.ai()
            .withDefaultLlm()
            .creating(ChannelEditPlan.class)
            .withExample(example)  // Pass the ObjectCreationExample directly
            .fromPrompt("Analyze the edit request");

    assertEquals(expectedPlan, result);
}
----

[source,kotlin]
----
@Test
fun `should use ObjectCreationExample data class`() {
    val context = FakeOperationContext.create()
    val expectedPlan = ChannelEditPlan(1, "Lead Vox")
    context.expectResponse(expectedPlan)

    // Create a reusable example using ObjectCreationExample
    val example = ObjectCreationExample(
        description = "Rename channel example",
        value = ChannelEditPlan(2, "Rhythm")
    )

    val result = context.ai()
        .withDefaultLlm()
        .creating(ChannelEditPlan::class.java)
        .withExample(example)  // Pass the ObjectCreationExample directly
        .fromPrompt("Analyze the edit request")

    assertEquals(expectedPlan, result)
}
----

**Adding Multiple Examples with withExamples():**

When you have many examples to add, use `withExamples()` to pass them as a list or vararg. This is especially useful when examples are loaded from a file or database:

[source,java]
----
@Test
void shouldAddMultipleExamplesFromList() {
    var context = FakeOperationContext.create();
    var expectedPlan = new ChannelEditPlan(1, "Lead Vox");
    context.expectResponse(expectedPlan);

    // Create a list of examples (could be loaded from configuration)
    var examples = List.of(
        new ObjectCreationExample<>("Rename to Bass", new ChannelEditPlan(1, "Bass")),
        new ObjectCreationExample<>("Rename to Drums", new ChannelEditPlan(2, "Drums")),
        new ObjectCreationExample<>("Rename to Keys", new ChannelEditPlan(3, "Keys")),
        new ObjectCreationExample<>("Rename to Vocals", new ChannelEditPlan(4, "Vocals"))
    );

    var result = context.ai()
            .withDefaultLlm()
            .creating(ChannelEditPlan.class)
            .withExamples(examples)  // Pass all examples at once
            .fromPrompt("Analyze the request");

    assertEquals(expectedPlan, result);

    // Verify all examples were added
    var promptContributors = context.getLlmInvocations().getFirst()
            .getInteraction().getPromptContributors();
    assertTrue(promptContributors.size() >= 4);
}
----

[source,kotlin]
----
@Test
fun `should add multiple examples from list`() {
    val context = FakeOperationContext.create()
    val expectedPlan = ChannelEditPlan(1, "Lead Vox")
    context.expectResponse(expectedPlan)

    // Create a list of examples (could be loaded from configuration)
    val examples = listOf(
        ObjectCreationExample("Rename to Bass", ChannelEditPlan(1, "Bass")),
        ObjectCreationExample("Rename to Drums", ChannelEditPlan(2, "Drums")),
        ObjectCreationExample("Rename to Keys", ChannelEditPlan(3, "Keys")),
        ObjectCreationExample("Rename to Vocals", ChannelEditPlan(4, "Vocals"))
    )

    val result = context.ai()
        .withDefaultLlm()
        .creating(ChannelEditPlan::class.java)
        .withExamples(examples)  // Pass all examples at once
        .fromPrompt("Analyze the request")

    assertEquals(expectedPlan, result)

    // Verify all examples were added
    val promptContributors = context.llmInvocations.first().interaction.promptContributors
    assertTrue(promptContributors.size >= 4)
}
----

You can also use vararg syntax for inline example lists:

[source,kotlin]
----
val result = context.ai()
    .withDefaultLlm()
    .creating(ChannelEditPlan::class.java)
    .withExamples(
        ObjectCreationExample("Example 1", ChannelEditPlan(1, "Bass")),
        ObjectCreationExample("Example 2", ChannelEditPlan(2, "Drums")),
        ObjectCreationExample("Example 3", ChannelEditPlan(3, "Keys"))
    )
    .fromPrompt("Analyze the request")
----

**Full Fluent API Chain Example:**

Here's a complete example showing how to test an action that uses all the fluent API features:

[source,java]
----
@Test
void shouldTestCompleteFluentApiChain() {
    var context = FakeOperationContext.create();
    var expectedOutput = new ComplexOutput("analysis complete", 42);
    context.expectResponse(expectedOutput);

    // Production code pattern with full fluent API chain
    var result = context.ai()
            .withLlm(LlmOptions.withModel("gpt-4"))
            .withId("complex-analysis")
            .withSystemPrompt("You are an expert analyst")
            .creating(ComplexOutput.class)
            .withExample("Simple case", new ComplexOutput("basic", 1))
            .withExample("Complex case", new ComplexOutput("advanced", 100))
            .fromPrompt("Analyze the input data");

    assertEquals(expectedOutput, result);

    // Comprehensive verification
    var invocation = context.getLlmInvocations().getFirst();
    assertEquals("gpt-4", invocation.getInteraction().getLlm().getModel());
    assertEquals("complex-analysis", invocation.getInteraction().getId().getValue());
    assertTrue(invocation.getInteraction().getPromptContributors().size() >= 3); // system + 2 examples
}
----

===== Key Testing Patterns Demonstrated

**Testing Prompt Content:**

- Use `context.getLlmInvocations().getFirst().getPrompt()` to get the actual prompt sent to the LLM
- Verify that key domain data is properly included in the prompt using `assertTrue(prompt.contains(...))`

**Testing Tool Group Configuration:**

- Access tool groups via `getInteraction().getToolGroups()`
- Verify expected tool groups are present or absent as required

**Testing with Spring Dependencies:**

- Mock Spring-injected services like `HoroscopeService` using standard mocking frameworks - Pass mocked dependencies to agent constructor for isolated unit testing

===== Testing Multiple LLM Interactions

[source,java]
----
@Test
void shouldHandleMultipleLlmInteractions() {
    // Arrange
    var input = new UserInput("Write about space exploration");
    var story = new Story("The astronaut gazed at Earth...");
    ReviewedStory review = new ReviewedStory("Compelling narrative with vivid imagery.");
    
    // Set up expected responses in order
    context.expectResponse(story);
    context.expectResponse(review);

    // Act
    var writtenStory = agent.writeStory(input, context);
    ReviewedStory reviewedStory = agent.reviewStory(writtenStory, context);

    // Assert
    assertEquals(story, writtenStory);
    assertEquals(review, reviewedStory);
    
    // Verify both LLM calls were made
    List<LlmInvocation> invocations = context.getLlmInvocations();
    assertEquals(2, invocations.size());
    
    // Verify first call (writer)
    var writerCall = invocations.get(0);
    assertEquals(0.8, writerCall.getInteraction().getLlm().getTemperature(), 0.01);
    
    // Verify second call (reviewer)
    var reviewerCall = invocations.get(1);
    assertEquals(0.2, reviewerCall.getInteraction().getLlm().getTemperature(), 0.01);
}
----

You can also use Mockito or mockk directory.
Consider this component, using direct injection of `Ai`:

[source,java]
----
@Component
public record InjectedComponent(Ai ai) {

    public record Joke(String leadup, String punchline) {
    }

    public String tellJokeAbout(String topic) {
        return ai
                .withDefaultLlm()
                .generateText("Tell me a joke about " + topic);
    }
}
----

A unit test using Mockito to verify prompt and hyperparameters:

[source,java]
----
class InjectedComponentTest {

    @Test
    void testTellJokeAbout() {
        var mockAi = Mockito.mock(Ai.class);
        var mockPromptRunner = Mockito.mock(PromptRunner.class);

        var prompt = "Tell me a joke about frogs";
        // Yep, an LLM came up with this joke.
        var terribleJoke = """
                Why don't frogs ever pay for drinks?
                Because they always have a tadpole in their wallet!
                """;
        when(mockAi.withDefaultLlm()).thenReturn(mockPromptRunner);
        when(mockPromptRunner.generateText(prompt)).thenReturn(terribleJoke);

        var injectedComponent = new InjectedComponent(mockAi);
        var joke = injectedComponent.tellJokeAbout("frogs");

        assertEquals(terribleJoke, joke);
        Mockito.verify(mockAi).withDefaultLlm();
        Mockito.verify(mockPromptRunner).generateText(prompt);
    }

}
----

==== Integration Testing

Integration testing exercises complete agent workflows with real or mock external services while still avoiding actual LLM calls for predictability and speed.

This can ensure:

- Agents are picked up by the agent platform
- Data flow is correct within agents
- Failure scenarios are handled gracefully
- Agents interact correctly with each other and external systems
- The overall workflow behaves as expected
- LLM prompts and hyperparameters are correctly configured

Embabel integration testing is built on top of https://docs.spring.io/spring-framework/reference/testing/integration.html[Spring's excellent integration testing support], thus allowing you to work with real databases if you wish.
Spring's https://docs.spring.io/spring-boot/reference/testing/testcontainers.html[integration with Testcontainers] is particularly userul.

===== Using EmbabelMockitoIntegrationTest

Embabel provides `EmbabelMockitoIntegrationTest` as a base class that simplifies integration testing with convenient helper methods:

[source,java]
----

/**
* Use framework superclass to test the complete workflow of writing and reviewing a story.
* This will run under Spring Boot against an AgentPlatform instance * that has loaded all our agents.
*/ class StoryWriterIntegrationTest extends EmbabelMockitoIntegrationTest {

    @Test
    void shouldExecuteCompleteWorkflow() {
        var input = new UserInput("Write about artificial intelligence");

        var story = new Story("AI will transform our world...");
        var reviewedStory = new ReviewedStory(story, "Excellent exploration of AI themes.", Personas.REVIEWER);

        whenCreateObject(contains("Craft a short story"), Story.class)
                .thenReturn(story);

        // The second call uses generateText
        whenGenerateText(contains("You will be given a short story to review"))
                .thenReturn(reviewedStory.review());

        var invocation = AgentInvocation.create(agentPlatform, ReviewedStory.class);
        var reviewedStoryResult = invocation.invoke(input);

        assertNotNull(reviewedStoryResult);
        assertTrue(reviewedStoryResult.getContent().contains(story.text()),
                "Expected story content to be present: " + reviewedStoryResult.getContent());
        assertEquals(reviewedStory, reviewedStoryResult,
                "Expected review to match: " + reviewedStoryResult);

        verifyCreateObjectMatching(prompt -> prompt.contains("Craft a short story"), Story.class,
                llm -> llm.getLlm().getTemperature() == 0.9 && llm.getToolGroups().isEmpty());
        verifyGenerateTextMatching(prompt -> prompt.contains("You will be given a short story to review"));
        verifyNoMoreInteractions();
    }
}
----

===== Key Integration Testing Features

**Base Class Benefits:**
- `EmbabelMockitoIntegrationTest` handles Spring Boot setup and LLM mocking automatically - Provides `agentPlatform` and `llmOperations` pre-configured - Includes helper methods for common testing patterns

**Convenient Stubbing Methods:**
- `whenCreateObject(prompt, outputClass)`: Mock object creation calls - `whenGenerateText(prompt)`: Mock text generation calls - Support for both exact prompts and `contains()` matching

**Advanced Verification:**
- `verifyCreateObjectMatching()`: Verify prompts with custom matchers - `verifyGenerateTextMatching()`: Verify text generation calls - `verifyNoMoreInteractions()`: Ensure no unexpected LLM calls

**LLM Configuration Testing:**
- Verify temperature settings: `llm.getLlm().getTemperature() == 0.9`
- Check tool groups: `llm.getToolGroups().isEmpty()`
- Validate persona and other LLM options

